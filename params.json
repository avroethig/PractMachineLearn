{"name":"Practmachinelearn","tagline":"","body":"---\r\ntitle: \"Practical Machine Learning - Course Project\"\r\nauthor: \"A RÃ¶thig\"\r\ndate: \"27 September 2015\"\r\noutput: html_document\r\n---\r\n\r\n## Summary\r\nThe following report presents a machine learning algorithm for classifying the activity quality of some Unilateral Dumbbell Biceps Curl exercises from some activity monitors recorded in the given dataset presented in [Ugulino, W. et.all (2012)].   \r\n\r\n## Dataset description\r\n\r\nThe quality of the exercise is quantified in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E). The set contains the sensor data from four sensors positioned in different places on the body of the participant and on the dumbell. The positions are belt, forearm, arm and dumbell. The variables of the dataset are features on the roll, pitch and yaw angles over a period of time, and the raw accelerometer, gyroscope and magnetometer readings. Features on the Euler angles are mean, variance,\r\nstandard deviation, max, min, amplitude, kurtosis and skewness. There were six participants that were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in the five different fashions.\r\n\r\n## Loading the data\r\n\r\nThe follwing code chunk downloads the data and loads it in two data frames.\r\n\r\n```{r expl1, eval = F}\r\n# download the data\r\nif (!file.exists(\"pml-training.csv\"))\r\n{\r\n  train_url <- \"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\" \r\n  download.file(train_url,destfile = \"pml-training.csv\")  \r\n}\r\n\r\nif (!file.exists(\"pml-testing.csv\"))\r\n{\r\n  test_url <- \"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\"\r\n  download.file(test_url,destfile = \"pml-testing.csv\")  \r\n}\r\n\r\n# load the data\r\nDF_train <- read.csv(\"pml-training.csv\")\r\nDF_test <- read.csv(\"pml-testing.csv\")\r\n\r\n# load packages\r\nrequire(caret)\r\nrequire(ggplot2)\r\nrequire(rattle)\r\nrequire(rpart)\r\nrequire(dplyr)\r\n```\r\n\r\n## Data processing\r\n\r\nThe following steps were taken in processing the data:\r\n\r\n1. Variables with zero-variance were taken out from the list of possible predictors.\r\n\r\n```{r proc1, eval = F}\r\n# check and throw out all variables with no variability\r\nnsv <- nearZeroVar(DF_train,saveMetrics=T)\r\nDF_train_subcol <- DF_train[rownames(nsv[nsv$nzv == F,])]\r\n```\r\n\r\n2. Factor variables containing the subject identification and time variables were also taken out from the list of possible predictors.\r\n\r\n```{r proc2,eval = F}\r\n# take out the non-predictors (user_name)\r\nDF_train_subcol.pred <- select(DF_train_subcol,-c(1:6))\r\n```\r\n\r\n3. From the 94 remaining (numeric) variables, the ones with many NA's were also taken out from the list of possible predictors.\r\n\r\n```{r proc3, eval =F}\r\n# take variables with not so many na's\r\npct_na <- unlist(lapply(DF_train_subcol.pred, function(x) mean(is.na(x))))\r\nDF_train_subcol.pred.clean <- DF_train_subcol.pred[,names(which(pct_na <= .1))]\r\n```\r\n\r\n4. The variables with a pair-wise correlation (absolute value) higher than .75 were computed, and the each time, the one with the largest mean absolute correlation was removed.\r\n\r\n```{r proc4,eval =F}\r\n# find correlated variables\r\ndescrCor <-  cor(DF_train_subcol.pred.clean[,-53],use = \"complete.obs\")\r\nsummary(descrCor[upper.tri(descrCor)])\r\n# take variables with max .75 correlation\r\nhighlyCorDescr <- findCorrelation(descrCor, cutoff = .75)\r\nDF_train_subcol.pred.clean <- DF_train_subcol.pred.clean[,-highlyCorDescr]\r\n\r\n# check\r\nlastcol <- ncol(DF_train_subcol.pred.clean)\r\ndescrCor2 <- cor(DF_train_subcol.pred.clean[,-lastcol])\r\nsummary(descrCor2[upper.tri(descrCor2)])\r\n```\r\n\r\n5. Splitting the data in a training and testing set.\r\n\r\n```{r split,eval = F}\r\n# split the given data in training and splitting data to avoid overfitting\r\ninTrain  <- createDataPartition(y=DF_train_subcol.pred.clean$classe, p= .75, list = F)\r\ntraining <- DF_train_subcol.pred.clean[ inTrain,]\r\ntesting  <- DF_train_subcol.pred.clean[-inTrain,]\r\n```\r\n\r\n## Training the model\r\n\r\nThe used method is Random Forest with (two times) repeated Cross-Validation.\r\n\r\n```{r training, eval = F}\r\n# training\r\ntrain_control <- trainControl(method=\"repeatedcv\", \r\n                                number = 10, repeats = 2,allowParallel = T)\r\n\r\n# train the model \r\nmodFit <- train(classe ~ ., data=training, \r\n               method=\"rf\", trControl=train_control)\r\n\r\n# print results\r\nprint(modFit$finalModel)\r\nmodFit\r\n```\r\n\r\n## Results\r\n\r\nThe classification results on the test data set are given in the following code chunk.\r\n\r\n```{r loaded res, echo = F, warning=F,message=F}\r\nrequire(caret)\r\nif (file.exists('environm.RData')) load('environm.RData')\r\npredictions <- predict(modFit, newdata = testing)\r\n\r\n```\r\n\r\n```{r resul, eval=T}\r\n# make predictions\r\npredictions <- predict(modFit, newdata = testing)\r\n# summarize results\r\nprint(confusionMatrix(predictions,testing$classe))\r\n```\r\n\r\n### Expected out-of-sample error\r\n\r\nAn estimate of the out-of-sample error rate can be given as the proportion of misclassified instances to the total number of instances. This is equal to 1 - Accuracy, given by the code chunk above. Thus, the estimate is 0.0075. Since the training algorithmus applied the repeated Cross-Validation method, the estimate 0.0075 is an estimate of the averaged out-of-sample error rate from each of the two repetitions (repeats = 2).\r\n\r\n## Literature\r\n\r\nUgulino, W.; Cardador, D.; Vega, K.; Velloso, E.; Milidiu, R.; Fuks, H. Wearable Computing: Accelerometers' Data Classification of Body Postures and Movements. Proceedings of 21st Brazilian Symposium on Artificial Intelligence. Advances in Artificial Intelligence - SBIA 2012. In: Lecture Notes in Computer Science. , pp. 52-61. Curitiba, PR: Springer Berlin / Heidelberg, 2012. ISBN 978-3-642-34458-9. DOI: 10.1007/978-3-642-34459-6_6.\r\n\r\nRead more: http://groupware.les.inf.puc-rio.br/har#ixzz3mvKimgZw\r\n\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}